{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd097a40",
   "metadata": {
    "id": "fd097a40"
   },
   "source": [
    "# Question1\n",
    " POETRY Generation using N-grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd8420",
   "metadata": {
    "id": "bbfd8420"
   },
   "source": [
    "1 Introduction:\n",
    "In this assignment, you will use n-gram language modeling to generate some poetry using the ngrams. For the purpose of this assignment a poem will consist of three stanzas each containing four verses where each verse consists of 7—10 words. For example, following is a manually generated stanza.\n",
    "\n",
    "دل سے نکال یاس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "ہوتا ہے کیوں اداس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "مایوسیوں کی قید سے خود کو نکال کر،\n",
    "\n",
    "آ جاؤ میرے پاس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "\n",
    "\n",
    "آ کر کبھی تو دید سے سیراب کر مجھے،\n",
    "\n",
    "مرتی نہیں ہے پیاس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "مہر و وفا خلوص و محبت گداز دل،\n",
    "\n",
    "سب کچھ ہے میرے پاس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "لوٹیں گے تیرے آتے ہی پھر دن بہار کے،\n",
    "\n",
    "رہتی ہے دل میں آس کہ زندہ ہوں میں،\n",
    "\n",
    "نایا ب شاخ چشم میں کھلتے ہیں اب بھی خواب، سچ ہے ترا\n",
    "\n",
    "قیاس کہ زندہ ہوں میں ابھی\n",
    "\n",
    "The task is to print three such stanzas with an empty line in between. The generation model can be trained on the provided Poetry Corpus containing poems from Faiz, Ghalib and Iqbal.You can scrape other urdu poetry too from internet. You will train unigram and bigram models using this corpus. These models will be used to generate poetry.\n",
    "\n",
    "2 Assignment Task:\n",
    "\n",
    "The task is to generate a poem using different models. We will generate a poem verse by verse until all stanzas have been generated. The poetry generation problem can be solved using the following algorithm:\n",
    "1. Load the Poetry Corpus\n",
    "2. Tokenize the corpus in order to split it into a list of words\n",
    "3. Generate n-gram models\n",
    "4. For each of the stanzas\n",
    "– For each verse\n",
    "* Generate a random number in the range [7...10]\n",
    "* Select first word\n",
    "* Select subsequent words until end of verse\n",
    "* [bonus] If not the first verse, try to rhyme the last word with the last word of the previous verse\n",
    "* Print verse\n",
    "– Print empty line after stanza\n",
    "2.1 Implementation Challenges:\n",
    "\n",
    "Among the challenges of solving this assignment will be selecting subsequent words once we have chosen the first word of the verse. To predict the next word, what we aim to compute is the most probable next word from all the possible next words. In other words, we need to find the set of words that occur most frequently after the already selected word and choose the next word from that set. We can use a Conditional Frequency Distribution (CFD) to figure that out! A CFD tells us: given a condition, what is likelihood of each possible outcome. [bonus] Rhyming the generated verses is also a challenge. You can build your dictionary for rhyming. The Urdu sentence is written from right to left, so makes your n-gram models according to this style.\n",
    "\n",
    "2.2 Standard n-gram Models\n",
    "We can develop our model using the Conditional Frequency Distribution method. First develop a unigram model (Unigram Model), then the bigram model (Bigram Model) and then trigram model. Select the first word of each line randomly from starting words in the vocabulary and then use the bigram model to generate the next word until the verse is complete. Generate the next three lines similarly.\n",
    " Follow the same steps for the trigram model and compare the results of the two n-gram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb709115",
   "metadata": {
    "id": "bb709115"
   },
   "outputs": [],
   "source": [
    "file1 = open('/home/maaz/Desktop/iqbal.txt','r',encoding='utf-8')\n",
    "file2 = open('/home/maaz/Desktop/ghalib.txt','r',encoding='utf-8')\n",
    "file1 = file1.read()\n",
    "file2 = file2.read()\n",
    "file = file1 + file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d44d157-c337-4d95-94ac-978d2ff92dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = \"\\n\"\n",
    "if space in file:\n",
    "    file = f\"{file.replace(space , ' ')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f25e641-413a-4598-959c-c4fb33fd6294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "words_token = nltk.word_tokenize(file)\n",
    "bgram = list(nltk.bigrams(words_token))\n",
    "tgram = list(nltk.trigrams(words_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "9d153a4c-3348-46a5-b19e-b08f5257412c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "دشوار\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(words_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "50482daa-0829-4106-9cdd-afcf123ac19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgram_freq = nltk.FreqDist(bgram)\n",
    "tgram_freq = nltk.FreqDist(tgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "1041edf1-412d-47df-9651-d956dfa1d398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the word محبت\n"
     ]
    }
   ],
   "source": [
    "input_word = input(\"Enter the word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "481d7932-e036-4b83-96b4-c866566486e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "next_word = input_word\n",
    "verse_line = []\n",
    "verse = []\n",
    "for i in range(12):\n",
    "    verse = []\n",
    "    if i == 0:\n",
    "        verse.append(input_word)\n",
    "    for j in range(10):\n",
    "        word = []\n",
    "        for bigram, count in bgram_freq.items():\n",
    "            n = []\n",
    "            if bigram[0] == next_word:\n",
    "                n.append(bigram[1])\n",
    "                n.append(count)\n",
    "                word.append(n)\n",
    "                word.sort(key=lambda x: int(x[1]),reverse=True)\n",
    "        rand_num = random.randint(0,len(word)-1)\n",
    "        verse.append(word[rand_num][0])\n",
    "        next_word = word[rand_num][0]\n",
    "        if j == 9:\n",
    "            next_word = random.choice(words_token)\n",
    "    verse_line.append(verse)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "e390c064-7da3-423e-823a-70756b79a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "next_word = input_word\n",
    "verse_line = []\n",
    "verse = []\n",
    "for i in range(12):\n",
    "    verse = []\n",
    "    if i == 0:\n",
    "        verse.append(input_word)\n",
    "    for j in range(10):\n",
    "        word = []\n",
    "        for bigram, count in bgram_freq.items():\n",
    "            n = []\n",
    "            if bigram[0] == next_word:\n",
    "                n.append(bigram[1])\n",
    "                n.append(count)\n",
    "                word.append(n)\n",
    "                word.sort(key=lambda x: int(x[1]),reverse=True)\n",
    "        num = 7\n",
    "        if num > len(word):\n",
    "            num = len(word)-1\n",
    "        rand_num = random.randint(0,num)\n",
    "        verse.append(word[rand_num][0])\n",
    "        next_word = word[rand_num][0]\n",
    "        if j == 9:\n",
    "            next_word = random.choice(words_token)\n",
    "            if i > 0:\n",
    "                n_wrd = verse_line[i-1][len(verse_line[i-1])-1]\n",
    "                n_word = n_wrd[-1]\n",
    "                while(True):\n",
    "                    wrd = random.choice(words_token)\n",
    "                    wrd1= wrd[-1]\n",
    "                    if wrd1 == n_word:\n",
    "                        break\n",
    "                verse[9] = wrd\n",
    "    verse_line.append(verse)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "91670c2c-d53e-4dfb-bf98-ae954262e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "محبت لکھوں تو دلبری  جلوہ کو امیدیں  قفس مرغ\n",
      "ہا دیوانہ پر کیوں کرتے ہیں مے سراسر رشتہٴ گوہر\n",
      "ہے آزمانا  ولے ان وعد اللہ رے ذوق معاصی\n",
      "سا منہ ہے خجل ہے تیغ آب و غم سے\n",
      "\n",
      "\n",
      "آنکھوں میں کھٹکے ہے دعوت مژگاں ہوں خم ہستی غافل\n",
      "بغداد یہ وہ غنچہ بہ جلوہ کو تعبیر کا کر\n",
      " جلوۂ تمثال کے کٹنے کا گلہ کیا کیجیے ہائے\n",
      "اٹھتی ہے انسان پر مجھ کو لاگ دوزخ کو مشاطۂ\n",
      "\n",
      "\n",
      "تیرا پتا نہ بخشا جاوے کف خاک یہ بندۂ مومن\n",
      "سوزودرد و کفن باندھے ہوئے پرندوں کی تمنا شکار کر\n",
      "ہے ملاقات کب تک ہر موج و قدح پہ پریشاں\n",
      "اے مسلماں  نگہہ معمار حسرت پرست  ہمیں دکھایا\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_file = []\n",
    "for i in range(12):\n",
    "    verse = verse_line[i]\n",
    "    r = ' '.join(verse)\n",
    "    r = ''.join(char for char in r if char.isalnum() or char.isspace())\n",
    "    new_file.append(r)\n",
    "    print(r,end = \"\\n\")\n",
    "    if i == 3 or i == 7 or i == 11:\n",
    "        new_file.append(\"\\n\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "d00463cc-4f5f-407f-bee6-9574ab253ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list has been saved to /home/maaz/Desktop/new_peom1.txt.\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/maaz/Desktop/new_peom1.txt'\n",
    "\n",
    "file = open(file_path, 'w')\n",
    "\n",
    "for item in new_file :\n",
    "    file.write(item + '\\n')\n",
    "\n",
    "file.close()\n",
    "\n",
    "print(f'The list has been saved to {file_path}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79601988",
   "metadata": {
    "id": "79601988"
   },
   "source": [
    "# Question2\n",
    " Classify language out of the list given below using just stop words. Remove punctuations, make it lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bdad25b",
   "metadata": {
    "id": "1bdad25b"
   },
   "outputs": [],
   "source": [
    "Test=\"An article is qualunque member van un class of dedicated words naquele estão used with noun phrases per mark the identifiability of the referents of the noun phrases\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "266654b6",
   "metadata": {
    "id": "266654b6",
    "outputId": "38cd33e4-19c0-4338-a6af-6b2951888fe9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arabic': 0,\n",
       " 'azerbaijani': 3,\n",
       " 'basque': 0,\n",
       " 'bengali': 0,\n",
       " 'catalan': 3,\n",
       " 'chinese': 0,\n",
       " 'danish': 0,\n",
       " 'dutch': 5,\n",
       " 'english': 9,\n",
       " 'finnish': 0,\n",
       " 'french': 1,\n",
       " 'german': 1,\n",
       " 'greek': 0,\n",
       " 'hebrew': 0,\n",
       " 'hinglish': 12,\n",
       " 'hungarian': 1,\n",
       " 'indonesian': 1,\n",
       " 'italian': 2,\n",
       " 'kazakh': 0,\n",
       " 'nepali': 0,\n",
       " 'norwegian': 0,\n",
       " 'portuguese': 1,\n",
       " 'romanian': 1,\n",
       " 'russian': 0,\n",
       " 'slovene': 0,\n",
       " 'spanish': 1,\n",
       " 'swedish': 0,\n",
       " 'tajik': 0,\n",
       " 'turkish': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "Test = Test.lower()\n",
    "Test = ''.join(char for char in Test if char.isalnum() or char.isspace())\n",
    "words = nltk.word_tokenize(Test)\n",
    "stp_lang = stopwords.fileids()\n",
    "df={}\n",
    "for k in range(len(stp_lang)):\n",
    "    df[stp_lang[k]] = 0\n",
    "for i in range(len(words)):\n",
    "    for j in range(len(stp_lang)):\n",
    "        if words[i] in stopwords.words(stp_lang[j]):\n",
    "            df[stp_lang[j]] +=1\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43708fa5",
   "metadata": {
    "id": "43708fa5"
   },
   "source": [
    "# Question 3\n",
    " Rule Based Roman Urdu Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f761e",
   "metadata": {
    "id": "2e1f761e"
   },
   "source": [
    "Roman Urdu lacks standard lexicon and usually many spelling variations exist for a given word, e.g., the word zindagi (life) is also written as zindagee, zindagy, zaindagee and zndagi. So, in this question you have to Normalize Roman Urdu words using the following Rules given in the attached Pdf. Your Code works for a complete Sentence or multiple sentences.\n",
    "\n",
    "For Example: zaroori, zaruri, zarori map to the 'zrory'. So zrory becomes the correct word for all representations mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd7e7159",
   "metadata": {
    "id": "dd7e7159"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chizein']\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"cheezain\"\n",
    "tokens = word_tokenize(text)\n",
    "for i in range(len(tokens)):\n",
    "    word = tokens[i]\n",
    "    tokens[i] = re.sub(r's+', 's', tokens[i])\n",
    "    tokens[i] = re.sub(r'a+', 'a', tokens[i])\n",
    "    tokens[i] = re.sub(r'j+', 'j', tokens[i])\n",
    "    tokens[i] = re.sub(r'o+', 'o', tokens[i])\n",
    "    tokens[i] = re.sub(r'ee+', 'i', tokens[i])\n",
    "    if tokens[i].endswith(\"ain\"):\n",
    "        tokens[i] = tokens[i].replace(\"ain\", \"ein\")\n",
    "    if tokens[i].endswith(\"ay\"):\n",
    "        tokens[i] = tokens[i].replace(\"ay\", \"e\")\n",
    "    if tokens[i].endswith(\"ey\"):\n",
    "        tokens[i] = tokens[i].replace(\"ey\", \"e\")\n",
    "    if tokens[i].endswith(\"ie\"):\n",
    "        tokens[i] = tokens[i].replace(\"ie\", \"y\")\n",
    "    if \"u\" in tokens[i]:\n",
    "        tokens[i] = tokens[i].replace(\"u\", \"o\")\n",
    "    if \"ai\" in tokens[i]:\n",
    "        tokens[i] = tokens[i].replace(\"ai\", \"ae\")\n",
    "    if  \"ar\" in tokens[i]:\n",
    "        w1 = word[2:]\n",
    "        w2 = word[0:2]\n",
    "        tokens[i] = w2 +  w1.replace(\"ar\",\"r\")\n",
    "    if  \"es\" in tokens[i]:\n",
    "        w1 = word[2:]\n",
    "        w2 = word[0:2]\n",
    "        tokens[i] = w2.replace(\"es\",\"is\") +w1\n",
    "    if  \"ry\" in tokens[i]:\n",
    "        w1 = word[:-2]\n",
    "        w2 = word[-2:]\n",
    "        tokens[i] = w1.replace(\"ry\",\"ri\") +w2\n",
    "    if  \"sy\" in tokens[i]:\n",
    "        w1 = word[:-2]\n",
    "        w2 = word[-2:]\n",
    "        tokens[i] = w1.replace(\"sy\",\"si\") +w2\n",
    "    if  \"ty\" in tokens[i]:\n",
    "        w1 = word[:-2]\n",
    "        w2 = word[-2:]\n",
    "        tokens[i] = w1.replace(\"ty\",\"ti\") +w2\n",
    "    if \"iy\" in tokens[i]:\n",
    "        w1 = word[word.find(\"iy\"):]\n",
    "        w2 = word[0:word.find(\"iy\")]\n",
    "        w1 = re.sub(r'y+', 'y', tokens[i])\n",
    "        w1 = w1.replace(\"iy\",\"I\")\n",
    "        tokens[i] = w2+w1\n",
    "    if \"ih\" in tokens[i]:\n",
    "        w1 = word[word.find(\"ih\"):]\n",
    "        w2 = word[0:word.find(\"ih\")]\n",
    "        w1 = re.sub(r'h+', 'h', tokens[i])\n",
    "        w1 = w1.replace(\"ih\",\"eh\")\n",
    "        tokens[i] = w2+w1\n",
    "        \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m7iy18K4vJYt",
   "metadata": {
    "id": "m7iy18K4vJYt"
   },
   "source": [
    "# Question 4\n",
    "In this question, you have been given two text files in Urdu. The first file contains an Urdu dictionary,\n",
    "which consists of a list of words. The second file contains sentences that do not have spaces between the\n",
    "words and are difficult to read.\n",
    "آجخودبخشہوں\n",
    "This sentence, without proper word segmentation, is difficult to understand. However, with proper word\n",
    "segmentation, the sentence can be separated into individual words:\n",
    "آج خود بخش ہوں\n",
    "This makes the sentence much easier to read and understand.\n",
    "\n",
    "\n",
    "This task is create spaces between words using\n",
    "\n",
    "*   unigrams\n",
    "*   bigram\n",
    "*   trigrams\n",
    "\n",
    "You can use the list of words file/dictionary provided in assignment 1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "142c871a",
   "metadata": {
    "id": "142c871a"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "file1 = open('/home/maaz/Desktop/words.txt','r',encoding='utf-8')\n",
    "file2 = open('/home/maaz/Desktop/word_test.txt','r',encoding='utf-8')\n",
    "file3 = open('/home/maaz/Desktop/bigram_words.txt','r',encoding='utf-8')\n",
    "file4 = open('/home/maaz/Desktop/stp.txt','r',encoding='utf-8')\n",
    "dictionary = []\n",
    "for line in file1:\n",
    "    word = line.strip()\n",
    "    dictionary.append(word)\n",
    "file1.close()\n",
    "for line in file3:\n",
    "    word = line.strip()\n",
    "    dictionary.append(word)\n",
    "file3.close()\n",
    "stp_words = []\n",
    "for line in file4:\n",
    "    word = line.strip()\n",
    "    stp_words.append(word)\n",
    "file4.close()\n",
    "file_cont = file2.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f4de0b8-f082-47aa-8759-f00e87c072e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only giving space to stp words\n",
    "fnd_word = ''\n",
    "for i in range(len(stp_words)):\n",
    "    if stp_words[i] in file_cont:\n",
    "        fnd_word = stp_words[i]\n",
    "file_cont = f\"{file_cont.replace(fnd_word,fnd_word + ' ')}\"\n",
    "file = open('/home/maaz/Desktop/out.txt', 'w')\n",
    "file.write(file_cont)\n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a8d62-825e-4981-b149-0a04343696ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
